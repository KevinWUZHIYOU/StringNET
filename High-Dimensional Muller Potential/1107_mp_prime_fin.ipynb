{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "58ihB-Z1O6ZZ",
        "outputId": "c2c3806a-748e-4886-eb3b-b8bcdf8a9799"
      },
      "outputs": [],
      "source": [
        "# This code aims to locatethe MEP by Deep NN method in terms of the extended Muellerpotential\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.linalg import qr\n",
        "import pickle\n",
        "\n",
        "#-------------------------------------------------\n",
        "mycase=\"mp_prime\"\n",
        "casenum=\"19\"\n",
        "\n",
        "\n",
        "prob_casenum=\"18\"\n",
        "\n",
        "\n",
        "all_path=\"/content/drive/MyDrive/Colab_Notebooks/2023_MEP/1107_data/\"\n",
        "\n",
        "path_s=all_path+mycase+\"_s_\"+casenum+\".pkl\"\n",
        "path_path=all_path+mycase+\"_path_\"+casenum+\".pkl\"\n",
        "path_cos=all_path+mycase+\"_cos_\"+casenum+\".pkl\"\n",
        "path_energy=all_path+mycase+\"_energy_\"+casenum+\".pkl\"\n",
        "path_force=all_path+mycase+\"_force_\"+casenum+\".pkl\"\n",
        "path_cosloss=all_path+mycase+\"_coslosss_\"+casenum+\".pkl\"\n",
        "path_Q=all_path+mycase[0:2]+\"_Q_\"+prob_casenum+\".pkl\"\n",
        "path_lmax=all_path+mycase+\"_lmax_\"+casenum+\".pkl\"\n",
        "path_lg=all_path+mycase+\"_lg_\"+casenum+\".pkl\"\n",
        "#-------------------------------------------------\n",
        "\n",
        "\n",
        "model_name= all_path+\"/model_MEP_\"+mycase+\"_\"+casenum+\".pth\"\n",
        "model_Parameters_name = all_path+\"model_MEP_\"+mycase[0:2]+\"_\"+prob_casenum+\"pretrain\"+\".pth\" # Path\n",
        "\n",
        "load_model = True\n",
        "batches = 120000 #iterations\n",
        "beta = 10\n",
        "learning_rate = 1e-3\n",
        "dimension = 10 #dimension\n",
        "ad=0\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device to train\")\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "# H = np.random.randn(dimension+ad,dimension+ad)\n",
        "# Q,R=qr(H)\n",
        "# Q_ten=torch.tensor(Q).to(device)\n",
        "#--------------------------------------------------\n",
        "# Q=np.eye(2)\n",
        "# Q_ten=torch.tensor(Q).to(device)\n",
        "#-------------------------------------------------------\n",
        "pkl_file = open(path_Q, 'rb')\n",
        "Q = pickle.load(pkl_file)\n",
        "pkl_file.close()\n",
        "Q_ten=torch.tensor(Q).to(device)\n",
        "\n",
        "\n",
        "\n",
        "# output = open(path_Q, 'wb')\n",
        "# pickle.dump(Q,output)\n",
        "# output.close()\n",
        "\n",
        "x_lb=-1.5\n",
        "x_ub=0.9\n",
        "y_lb=-0.5\n",
        "y_ub=1.9\n",
        "\n",
        "alpha1=1\n",
        "alpha2=10\n",
        "alpha3=2.5\n",
        "\n",
        "def V_tensor_plot(x):\n",
        "  a = [ -1, -1, -6.5, 0.7]\n",
        "  b = [0, 0, 11, 0.6]\n",
        "  c = [ -10, -10, -6.5, 0.7]\n",
        "  D = [ -200, -100, -170, 15]\n",
        "  X = [1, 0, -0.5, -1]\n",
        "  Y = [0, 0.5, 1.5, 1]\n",
        "  sigma=0.5\n",
        "\n",
        "  res=0\n",
        "  for i in range(4):\n",
        "    res=res+D[i]*torch.exp(a[i]*(x[0]-X[i])**2+b[i]*(x[0]-X[i])*(x[1]-Y[i])+c[i]*(x[1]-Y[i])**2)\n",
        "  for i in range(dimension-2):\n",
        "    res=res+1/(2*sigma**2)*x[i+2]**2\n",
        "  return res\n",
        "\n",
        "\n",
        "def V_tensor_and_grad(x): # first order derivative of V w.r.t x\n",
        "    global Q_ten\n",
        "    a = [ -1, -1, -6.5, 0.7]\n",
        "    b = [0, 0, 11, 0.6]\n",
        "    c = [ -10, -10, -6.5, 0.7]\n",
        "    D = [ -200, -100, -170, 15]\n",
        "    X = [1, 0, -0.5, -1]\n",
        "    Y = [0, 0.5, 1.5, 1]\n",
        "    sigma=0.5\n",
        "\n",
        "    xx=torch.cat(x,dim=1)\n",
        "    xx=torch.matmul(xx,Q_ten)\n",
        "    x_temp=xx[:,0:dimension]\n",
        "    #print(\"x_new=\",x_temp)\n",
        "\n",
        "    x_all=[]\n",
        "    for i in range(dimension):\n",
        "      x_all.append(x_temp[:,i:i+1])\n",
        "\n",
        "    res=0\n",
        "    for i in range(4):\n",
        "      res=res+D[i]*torch.exp(a[i]*(x_all[0]-X[i])**2+b[i]*(x_all[0]-X[i])*(x_all[1]-Y[i])+c[i]*(x_all[1]-Y[i])**2)\n",
        "    for i in range(dimension-2):\n",
        "      res=res+1/(2*sigma**2)*x_all[i+2]**2\n",
        "\n",
        "    grad_Vx=[]\n",
        "    for i in range(dimension+ad):\n",
        "      grad_Vx.append(torch.autograd.grad(outputs=res, inputs=x[i], grad_outputs=torch.ones_like(res), create_graph=True)[0])\n",
        "\n",
        "    return res,grad_Vx\n",
        "\n",
        "\n",
        "def fig_cos_V_force(s,cos2,g,x_pred_list):\n",
        "    fig = plt.figure(figsize=(15,4))\n",
        "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.plot(s,cos2,'b.')\n",
        "    plt.ylim(-0.1,1.1)\n",
        "    plt.title(\"$Cos^2$\")\n",
        "\n",
        "    vplot,gg =V_tensor_and_grad(x_pred_list)\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(s,vplot.cpu().detach().numpy(), 'b.')\n",
        "    plt.title(\"$Energy$\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "\n",
        "    force=np.sqrt(np.sum(g*g, axis=1))\n",
        "    plt.plot(s,force, 'b.')\n",
        "    plt.title(\"$Force$\")\n",
        "    #plt.savefig(\"csf_\"+str(iter)+\".jpg\")\n",
        "\n",
        "def fig_loss_batch(plt_batch,loss_batch):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(plt_batch,loss_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    #plt.savefig(\"lb_\"+str(iter)+\".jpg\")\n",
        "\n",
        "def fig_cos(plt_batch,cos_batch,lmax_batch,lg_batch):\n",
        "    fig = plt.figure(figsize=(15,4))\n",
        "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.semilogy(plt_batch,cos_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"$\\int 1-Cos^2(force,tangent) ds$\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(plt_batch,lmax_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"$lmax$\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.plot(plt_batch,lg_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"$l_g$\")\n",
        "\n",
        "def fig_countour(x_pred):\n",
        "    global Q_ten\n",
        "    plt.figure(figsize=(5,4))\n",
        "    x = np.linspace(x_lb, x_ub,num=50,endpoint=True)#50\n",
        "    y = np.linspace(y_lb, y_ub,num=50,endpoint=True)\n",
        "    X,Y = np.meshgrid(x,y)\n",
        "    X_new=X.reshape(-1,1)\n",
        "    Y_new=Y.reshape(-1,1)\n",
        "    XY=np.hstack((X_new,Y_new))\n",
        "    for i in range(dimension-2):\n",
        "      XY=np.hstack((XY,np.zeros(X_new.shape)))\n",
        "\n",
        "    XY_tensor=torch.from_numpy(XY)\n",
        "    X_list = []\n",
        "    for i in range(dimension+ad):\n",
        "      X_list.append(XY_tensor[:, i:i+1])\n",
        "\n",
        "    Z = V_tensor_plot(X_list)\n",
        "    Z_new=Z.reshape(X.shape).cpu().detach().numpy()\n",
        "\n",
        "    plt.figure()\n",
        "    CS = plt.contourf(X,Y,Z_new,50,cmap=mpl.cm.jet)\n",
        "    plt.colorbar(CS)\n",
        "\n",
        "    #print(x_pred)\n",
        "    #xx=torch.cat(x,dim=1)\n",
        "    xx=torch.matmul(x_pred,Q_ten)\n",
        "    x_temp=xx[:,0:dimension]\n",
        "\n",
        "    # print(x_temp)\n",
        "    # input()\n",
        "\n",
        "    x_plot=x_temp.cpu().detach().numpy()\n",
        "    plt.plot(x_plot[:,0],x_plot[:,1],\"r.\",markersize=5)\n",
        "    plt.xlim((x_lb,x_ub))\n",
        "    plt.ylim((y_lb,y_ub))\n",
        "    plt.xlabel('$x_1$')\n",
        "    plt.ylabel('$x_2$')\n",
        "    # print(X,Y,Z_new)\n",
        "    # input()\n",
        "    #plt.savefig(\"path_\"+str(iter)+\".jpg\")\n",
        "\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self,st,ed):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(1, 32),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, dimension+ad)\n",
        "        )\n",
        "        self.startpoint=Variable(torch.from_numpy(st),requires_grad=False).to(device)\n",
        "        self.endpoint=Variable(torch.from_numpy(ed),requires_grad=False).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, s):\n",
        "        s = self.flatten(s)\n",
        "        x_pred = self.linear_tanh_stack(s)\n",
        "        out=s*(1-s)*x_pred + (1-s)*self.startpoint + s*self.endpoint\n",
        "        return out\n",
        "\n",
        "\n",
        "def train(model):\n",
        "    loss_batch=[]\n",
        "    plt_batch=[]\n",
        "    cos_batch=[]\n",
        "\n",
        "    lg_batch=[]\n",
        "    lmax_batch=[]\n",
        "\n",
        "    if load_model:\n",
        "        model.load_state_dict(torch.load(model_Parameters_name))\n",
        "\n",
        "    for i in range(batches):\n",
        "      #-------------1-------------\n",
        "        # if i<1000:\n",
        "        #   alpha1=10\n",
        "        #   alpha2=1\n",
        "        #   alpha3=1\n",
        "        # else:\n",
        "        #   alpha1=1\n",
        "        #   alpha2=10\n",
        "        #   alpha3=1\n",
        "        # alpha1=1\n",
        "        # alpha2=10\n",
        "        # alpha3=1\n",
        "\n",
        "        # learning_rate=1e-4\n",
        "\n",
        "      #-------------2------------\n",
        "        # if i<5000:\n",
        "        #   alpha1=1\n",
        "        #   alpha2=10\n",
        "        #   alpha3=1\n",
        "        # else:\n",
        "        #   alpha1=0.1\n",
        "        #   alpha2=10\n",
        "        #   alpha3=1\n",
        "\n",
        "        # learning_rate=1e-4\n",
        "\n",
        "      #-------------3------------\n",
        "        # if i<5000:\n",
        "        #   alpha1=1\n",
        "        #   alpha2=10\n",
        "        #   alpha3=1\n",
        "        # else:\n",
        "        #   alpha1=0.01\n",
        "        #   alpha2=10\n",
        "        #   alpha3=1\n",
        "      # #-------------4,5------------\n",
        "      #   if i<5000:\n",
        "      #     alpha1=1\n",
        "      #     alpha2=10\n",
        "      #     alpha3=1\n",
        "      #   else:\n",
        "      #     alpha1=0.1\n",
        "      #     alpha2=10\n",
        "      #     alpha3=1\n",
        "\n",
        "      #   if i<50000:\n",
        "      #     learning_rate=1e-4\n",
        "      #   else:\n",
        "      #     learning_rate=1e-5\n",
        "      # #-------------9------------\n",
        "      #   if i<5000:\n",
        "      #     alpha1=1\n",
        "      #     alpha2=10\n",
        "      #     alpha3=1\n",
        "      #   else:\n",
        "      #     alpha1=0.1\n",
        "      #     alpha2=10\n",
        "      #     alpha3=1\n",
        "\n",
        "      #   if i<60000:\n",
        "      #     learning_rate=1e-4\n",
        "      #   else:\n",
        "      #     learning_rate=1e-5\n",
        "      #-------------11------------\n",
        "        # if i<5000:\n",
        "        #   alpha1=1\n",
        "        #   alpha2=10\n",
        "        #   alpha3=1\n",
        "        # else:\n",
        "        #   alpha1=0.1\n",
        "        #   alpha2=10\n",
        "        #   alpha3=1\n",
        "\n",
        "        # if i<50000:\n",
        "        #   learning_rate=1e-4\n",
        "        # else:\n",
        "        #   learning_rate=1e-5\n",
        "      #-------------12------------\n",
        "      # If not work, alpha4=0.01\n",
        "      # # or use 5e-4 for i<20000\n",
        "      #   if i<5000:\n",
        "      #     alpha1=1\n",
        "      #     alpha4=0.001\n",
        "      #     alpha3=0.1\n",
        "      #   else:\n",
        "      #     alpha1=0.1\n",
        "      #     alpha4=10\n",
        "      #     alpha3=0.1\n",
        "\n",
        "      #   if i<50000:\n",
        "      #     learning_rate=1e-4\n",
        "      #   else:\n",
        "      #     learning_rate=1e-5\n",
        "    # -------------13----------------\n",
        "        # if i<5000:\n",
        "        #   alpha1=1\n",
        "        #   alpha4=0.001\n",
        "        #   alpha3=0.1\n",
        "        # elif i<40000:\n",
        "        #   alpha1=0.1\n",
        "        #   alpha4=10\n",
        "        #   alpha3=0.1\n",
        "        # else:\n",
        "        #   alpha1=0.001\n",
        "        #   alpha4=10\n",
        "        #   alpha3=0.1\n",
        "\n",
        "        # if i<50000:\n",
        "        #   learning_rate=1e-4\n",
        "        # else:\n",
        "        #   learning_rate=1e-5\n",
        "    #------------14------------\n",
        "        # if i<5000:\n",
        "        #   alpha1=1\n",
        "        #   alpha4=0.001\n",
        "        #   alpha3=0.1\n",
        "        # elif i<30000:\n",
        "        #   alpha1=0.1\n",
        "        #   alpha4=10\n",
        "        #   alpha3=0.1\n",
        "        # else:\n",
        "        #   alpha1=0\n",
        "        #   alpha4=10\n",
        "        #   alpha3=0.1\n",
        "\n",
        "        # if i<50000:\n",
        "        #   learning_rate=1e-4\n",
        "        # else:\n",
        "        #   learning_rate=1e-5\n",
        "    #------------18------------\n",
        "        if i<10000:\n",
        "          alpha1=1\n",
        "          alpha4=1\n",
        "          alpha3=0.001#smaller? 0.1works\n",
        "        elif i<30000:\n",
        "          alpha1=1\n",
        "          alpha4=1\n",
        "          alpha3=0.001\n",
        "        else:\n",
        "          alpha1=0 #0.1\n",
        "          alpha4=1\n",
        "          alpha3=0.001\n",
        "\n",
        "        learning_rate=1e-4\n",
        "\n",
        "\n",
        "\n",
        "        # if i<40000:\n",
        "        #   learning_rate=1e-4\n",
        "        # else:\n",
        "        #   learning_rate=1e-5\n",
        "\n",
        "        # if i<2000:\n",
        "        #   learning_rate=1e-3\n",
        "        # elif i<20000:# 10000\n",
        "        #   learning_rate=1e-4\n",
        "        # else:\n",
        "        #   learning_rate=1e-5\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        # Generate the training data\n",
        "\n",
        "        s = np.random.uniform(0.001,0.999,(250,1))\n",
        "        #s=np.array([[0.0],[1.0]])\n",
        "        s = Variable(torch.from_numpy(s),requires_grad=True).to(device)\n",
        "        x_pred = model(s)\n",
        "        #print(\"x_ori=\",x_pred)\n",
        "        x_pred_list = []\n",
        "        for n in range(dimension+ad):\n",
        "            x_pred_list.append(x_pred[:, n:n+1])\n",
        "\n",
        "        #loss_1\n",
        "        # print(\"flag1\")\n",
        "        #g_all = V_tensor_grad(x_pred_list)\n",
        "        v,g_all = V_tensor_and_grad(x_pred_list)\n",
        "        #input()\n",
        "        # print(\"v=\",v)\n",
        "        # print(\"g=\",g_all)\n",
        "        gradx_list=[]\n",
        "        gradgx_list=[]\n",
        "        for n in range(dimension+ad):\n",
        "            gradx_list.append(torch.autograd.grad(outputs=x_pred_list[n],inputs=s,grad_outputs=torch.ones_like(s),create_graph=True)[0]) # First order derivative of x w.r.t s\n",
        "            gradgx_list.append(torch.autograd.grad(outputs=gradx_list[n],inputs=s,grad_outputs=torch.ones_like(s),create_graph=True)[0]) # Second order derivative of x w.r.t s\n",
        "\n",
        "        partial_s_x = torch.cat(gradx_list, axis=1)\n",
        "        dot_partial_s_x = torch.sqrt(torch.sum(partial_s_x*partial_s_x, axis=1, keepdim=False)) #s对神经网络的一阶导数的数量积\n",
        "\n",
        "        loss_1=1/beta*torch.log(torch.mean(torch.exp(v*beta)))\n",
        "        #loss_1=1/beta*torch.log(torch.mean(torch.exp(v*beta)*dot_partial_s_x))\n",
        "\n",
        "        # print(\"flag2\")\n",
        "        #loss_2\n",
        "\n",
        "        # print(\"flag3\")\n",
        "        g=torch.cat(g_all, axis=1)\n",
        "        cos2 = (torch.sum(partial_s_x*g,axis=1, keepdim=True)/(torch.sum(g*g, axis=1, keepdim=True)**0.5*torch.sum(partial_s_x*partial_s_x, axis=1, keepdim=True)**0.5))**2\n",
        "        loss_2 = torch.mean(1-cos2)\n",
        "\n",
        "        #loss_3\n",
        "\n",
        "        # print(\"flag3\")\n",
        "        cons=0\n",
        "        for n in range(dimension+ad):\n",
        "            cons += torch.sum(gradx_list[n]*gradgx_list[n], axis=1, keepdim=True)\n",
        "        loss_3 = torch.mean(cons**2)\n",
        "\n",
        "        # print(\"loss1=\",loss_1,\"loss2=\",loss_2,\"loss3=\",loss_3)\n",
        "        # input()\n",
        "\n",
        "        #-----------------loss_4-----------------\n",
        "        dot_g= torch.sqrt(torch.sum(g*g, axis=1, keepdim=False))\n",
        "        loss_4=torch.mean(dot_g*dot_partial_s_x)\n",
        "\n",
        "\n",
        "        #loss总和\n",
        "\n",
        "        loss = alpha1*loss_1 +alpha4*loss_4+alpha3*loss_3\n",
        "        # alpha1=0\n",
        "        # loss = alpha4*loss_4+alpha3*loss_3\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i) % 50 == 0:\n",
        "            #print(\"50505050\")\n",
        "          #-------------------------------------------------------------\n",
        "            ss = np.linspace(0.001,0.999,1000,endpoint=True)#!!!!!!!!!!!Note this\n",
        "\n",
        "            ss = ss.reshape(-1,1)\n",
        "            ss = Variable(torch.from_numpy(ss),requires_grad=True).to(device)\n",
        "            xx = model(ss)\n",
        "            gradx_list_plot=[]\n",
        "            x_pred_list_plot = []\n",
        "            for n in range(dimension+ad):\n",
        "                x_pred_list_plot.append(xx[:, n:n+1])\n",
        "            for n in range(dimension+ad):\n",
        "                gradx_list_plot.append(torch.autograd.grad(outputs=x_pred_list_plot[n],inputs=ss,grad_outputs=torch.ones_like(ss),create_graph=True)[0]) # First order derivative of x w.r.t s\n",
        "            partial_s_x_plot = torch.cat(gradx_list_plot, axis=1)\n",
        "\n",
        "            VVV,g_all_plot = V_tensor_and_grad(x_pred_list_plot)\n",
        "            g_plot=torch.cat(g_all_plot, axis=1)\n",
        "            cos2_plot = (torch.sum(partial_s_x_plot*g_plot,axis=1, keepdim=True)/(torch.sum(g_plot*g_plot, axis=1, keepdim=True)**0.5*torch.sum(partial_s_x_plot*partial_s_x_plot, axis=1, keepdim=True)**0.5))**2\n",
        "            cos_plot_loss = torch.mean(1-cos2_plot)\n",
        "            cos_batch.append(cos_plot_loss.item())\n",
        "\n",
        "            lmax_batch.append(torch.max(VVV).item())\n",
        "\n",
        "            dot_partial_s_x_plot = torch.sqrt(torch.sum(partial_s_x_plot*partial_s_x_plot, axis=1, keepdim=False))\n",
        "            dot_g_plot= torch.sqrt(torch.sum(g_plot*g_plot, axis=1, keepdim=False))\n",
        "            lg=torch.mean(dot_g_plot*dot_partial_s_x_plot)\n",
        "\n",
        "            lg_batch.append(lg.item())\n",
        "          #-------------------------------------------------------------\n",
        "\n",
        "            loss, batch = alpha1*loss_1.item() + alpha4*loss_4.item() + alpha3*loss_3.item(), i\n",
        "            print(f'batches: {batch+1}')\n",
        "            print(f'loss1: {alpha1*loss_1} loss4: {alpha4*loss_4} loss3: {alpha3*loss_3}')\n",
        "            loss_batch.append(loss)\n",
        "            plt_batch.append(i+1)\n",
        "            plt.figure()\n",
        "\n",
        "        if (i+1) % 500 == 0:\n",
        "            # Maybe not using the data point to draw figures\n",
        "            # countor for V\n",
        "            print(\"cos_plot_loss.item()=\",cos_plot_loss.item())\n",
        "            fig_cos(plt_batch,cos_batch,lmax_batch,lg_batch)\n",
        "            fig_loss_batch(plt_batch,loss_batch)\n",
        "            fig_cos_V_force(s.cpu().detach().numpy(),cos2.cpu().detach().numpy(),g.cpu().detach().numpy(),x_pred_list)  # Why [1:-1\n",
        "            fig_countour(x_pred)\n",
        "            plt.show()\n",
        "\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "            print(\"Saved PyTorch Model State to \" +\"model_MEP.pth\")\n",
        "\n",
        "    output = open(path_s, 'wb')\n",
        "    pickle.dump(s.cpu().detach().numpy(),output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_path, 'wb')\n",
        "    pickle.dump(x_pred.cpu().detach().numpy(),output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_cos, 'wb')\n",
        "    pickle.dump(cos2.cpu().detach().numpy(),output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_energy, 'wb')\n",
        "    VV,gg=V_tensor_and_grad(x_pred_list)\n",
        "    pickle.dump(VV.cpu().detach().numpy(),output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_force, 'wb')\n",
        "    pickle.dump(np.sqrt(np.sum(g.cpu().detach().numpy()*g.cpu().detach().numpy(), axis=1)),output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_cosloss, 'wb')\n",
        "    pickle.dump(cos_batch,output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_lg, 'wb')\n",
        "    pickle.dump(lg_batch,output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_lmax, 'wb')\n",
        "    pickle.dump(lmax_batch,output)\n",
        "    output.close()\n",
        "    return\n",
        "\n",
        "def train_pre(model):\n",
        "    for i in range(0):\n",
        "      a=np.array([-81.0640995,70.67714575])\n",
        "      b=np.array([69.43047688,-67.71500979])\n",
        "\n",
        "      c=(a+b)/2\n",
        "      r=np.sqrt(np.sum((a-b)**2))/2\n",
        "\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "      t = np.random.uniform(0,1,(500,1))\n",
        "      t = Variable(torch.from_numpy(t),requires_grad=True).to(device)\n",
        "\n",
        "      xy=model(t)\n",
        "      x,y=torch.split(xy,[1,1],dim=1)\n",
        "      # print(x)\n",
        "      # print(c[0]+r*torch.cos(np.pi*t))\n",
        "      # input()\n",
        "\n",
        "      loss=torch.mean((x-c[0]-r*torch.cos(np.pi*t+2.398))**2+(y-c[1]-r*torch.sin(np.pi*t+2.398))**2)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if i % 500==0:\n",
        "        print(loss.item())\n",
        "    torch.save(model.state_dict(), model_Parameters_name)\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__=='__main__':\n",
        "    MEPpass_start_point=[6.23499399e-01,2.80377512e-02]+[0]*(dimension+ad-2)\n",
        "    MEPpass_end_point=[-5.58223673e-01,1.44172580e+00]+[0]*(dimension+ad-2)\n",
        "\n",
        "    MEPpass_start_point=np.array(MEPpass_start_point)\n",
        "    MEPpass_end_point=np.array(MEPpass_end_point)\n",
        "\n",
        "    MEPpass_start_point_new=np.matmul(Q,MEPpass_start_point.reshape(-1,1))\n",
        "    MEPpass_end_point_new=np.matmul(Q,MEPpass_end_point.reshape(-1,1))\n",
        "\n",
        "\n",
        "\n",
        "    model = NeuralNetwork(MEPpass_start_point_new.reshape(1,-1),MEPpass_end_point_new.reshape(1,-1)).to(device).double()\n",
        "    model = train_pre(model)\n",
        "    train(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh_emUD3Sssb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}