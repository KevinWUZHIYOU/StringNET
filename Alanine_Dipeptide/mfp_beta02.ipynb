{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "os.environ[\"PATH\"] = \"/usr/local/gromacs/bin:\" + os.environ[\"PATH\"]\n",
    "os.environ[\"PLUMED_KERNEL\"] = \"/Users/wuzhiyou/plumed/lib/libplumedKernel.dylib\"\n",
    "\n",
    "def restrained_simulation(grid_point):\n",
    "    phi_deg, psi_deg, phi, psi = grid_point\n",
    "    dir_name = f\"phi_{phi_deg}_psi_{psi_deg}\"\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "    # Generate plumed.dat\n",
    "    plumed_content_initial = f\"\"\"\n",
    "MOLINFO STRUCTURE=./ala2.pdb\n",
    "\n",
    "phi: TORSION ATOMS=@phi-2\n",
    "psi: TORSION ATOMS=@psi-2\n",
    "restraint: RESTRAINT ARG=phi,psi AT={phi},{psi} KAPPA=100.0,100.0\n",
    "\n",
    "PRINT ARG=phi,psi,restraint.bias FILE={dir_name}/COLVAR_initial STRIDE=100\n",
    "\"\"\"\n",
    "\n",
    "    plumed_content = f\"\"\"\n",
    "MOLINFO STRUCTURE=./ala2.pdb\n",
    "\n",
    "phi: TORSION ATOMS=@phi-2\n",
    "psi: TORSION ATOMS=@psi-2\n",
    "restraint: RESTRAINT ARG=phi,psi AT={phi},{psi} KAPPA=1000.0,1000.0\n",
    "\n",
    "PRINT ARG=phi,psi,restraint.bias FILE={dir_name}/COLVAR_restrained STRIDE=100\n",
    "\"\"\"\n",
    "    with open(f\"{dir_name}/plumed.dat\", \"w\") as f:\n",
    "        f.write(plumed_content)\n",
    "\n",
    "    with open(f\"{dir_name}/plumed_initial.dat\", \"w\") as f:\n",
    "        f.write(plumed_content_initial)\n",
    "\n",
    "    # Copy input files\n",
    "\n",
    "    # Run simulation\n",
    "    subprocess.run(\n",
    "        f\"gmx mdrun -s nvt.tpr -plumed {dir_name}/plumed_initial.dat -ntmpi 1 -deffnm {dir_name}/initial -v\",\n",
    "        shell=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        f\"gmx grompp -f nvt.mdp -c {dir_name}/initial.gro -r {dir_name}/initial.gro -p topol.top -o {dir_name}/restrained.tpr\",\n",
    "        shell=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        f\"gmx mdrun -s {dir_name}/restrained.tpr -plumed {dir_name}/plumed.dat  -ntmpi 1 -deffnm {dir_name}/restrained -v\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "# Create grid\n",
    "\n",
    "\n",
    "def plot_colvar(filename, figname):\n",
    "    colvar = np.loadtxt(filename, comments=\"#\")\n",
    "    deg_to_rad = np.pi / 180.0\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(colvar[:, 1]/deg_to_rad, colvar[:, 2]/deg_to_rad, c=colvar[:, 3], alpha=0.5)\n",
    "    plt.colorbar(label='Restraint Bias')\n",
    "    plt.xlabel('phi (rad)')\n",
    "    plt.ylabel('psi (rad)')\n",
    "    plt.title('COLVAR Analysis')\n",
    "    plt.savefig(figname)\n",
    "    plt.close()\n",
    "\n",
    "def FES_gradient(grid_point):\n",
    "    phi_deg, psi_deg, phi, psi = grid_point\n",
    "    dir_name = f\"phi_{phi_deg}_psi_{psi_deg}\"\n",
    "    colvar = np.loadtxt(f\"{dir_name}/COLVAR_restrained\", comments=\"#\")\n",
    "    phi_restrained = colvar[:, 1]\n",
    "    psi_restrained = colvar[:, 2]\n",
    "    bias = colvar[:, 3]\n",
    "    dphi = phi_restrained - phi\n",
    "    dpsi = psi_restrained - psi\n",
    "\n",
    "    while np.any(dphi >= np.pi):\n",
    "        dphi[dphi >= np.pi] -= 2 * np.pi\n",
    "    while np.any(dphi <= -np.pi):\n",
    "        dphi[dphi <= -np.pi] += 2 * np.pi\n",
    "    while np.any(dpsi >= np.pi):\n",
    "        dpsi[dpsi >= np.pi] -= 2 * np.pi\n",
    "    while np.any(dpsi <= -np.pi):\n",
    "        dpsi[dpsi <= -np.pi] += 2 * np.pi\n",
    "\n",
    "    gradient_phi = -1000.0 * np.mean(dphi)\n",
    "    gradient_psi = -1000.0 * np.mean(dpsi)\n",
    "\n",
    "    return gradient_phi, gradient_psi\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the directory (use \".\" for current directory)\n",
    "def delete_backupfile():\n",
    "    directory = \".\"\n",
    "\n",
    "    # Define the pattern for the backup files\n",
    "    backup_pattern = os.path.join(directory, \"bck.*.PLUMED.OUT\")\n",
    "\n",
    "    # Find all files matching the backup pattern\n",
    "    backup_files = glob.glob(backup_pattern)\n",
    "\n",
    "    # Iterate over the backup files and delete each one\n",
    "    for file_path in backup_files:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")\n",
    "\n",
    "    print(\"Done deleting backup files.\")\n",
    "\n",
    "step = 10\n",
    "deg_to_rad = np.pi / 180.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f80b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "device = 'cpu'\n",
    "\n",
    "# Define the file path to your CSV file.\n",
    "file_path = '/Users/wuzhiyou/Code/StringNET/data_standard_FF6/fes_gradients.csv'\n",
    "gradients_df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"First few rows of the mean force data:\")\n",
    "print(gradients_df.head())\n",
    "data = np.array(gradients_df)\n",
    "gradients_df = gradients_df[['phi','psi', 'grad_phi','grad_psi']]  # Keep only these two columns\n",
    "data = np.array(gradients_df)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float64  # We work in double precision\n",
    "\n",
    "class GPWithGradients:\n",
    "    def __init__(self, train_data: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_data: Torch tensor of shape (N, 4) with columns:\n",
    "                        [phi, psi, grad_phi, grad_psi].\n",
    "        \"\"\"\n",
    "        self.phi_train = train_data[:, 0].clone()\n",
    "        self.psi_train = train_data[:, 1].clone()\n",
    "        self.grad_phi_train = train_data[:, 2].clone()\n",
    "        self.grad_psi_train = train_data[:, 3].clone()\n",
    "        self.N = self.phi_train.shape[0]\n",
    "        self.alpha = None  \n",
    "        self.L = None  # Cholesky factor (if desired to be reused)\n",
    "        self.sigma_f = torch.nn.Parameter(torch.tensor(1.0, dtype=dtype, device=device))\n",
    "        self.l = torch.nn.Parameter(torch.tensor(1.0, dtype=dtype, device=device))\n",
    "\n",
    "    def __call__(self, test_input: torch.Tensor, noise=1e-6):\n",
    "        \"\"\"Makes the instance callable by delegating to the forward method.\"\"\"\n",
    "        return self.predict(test_input, noise)\n",
    "    \n",
    "    def kernel(self, phi1, psi1, phi2, psi2):\n",
    "        \"\"\"\n",
    "        Computes the periodic kernel between two sets of inputs.\n",
    "        The kernel is defined as:\n",
    "            k((phi, psi), (phi', psi')) = sigma_f^2 * exp( -(2 - cos(phi-phi') - cos(psi-psi')) / l^2 )\n",
    "        Inputs:\n",
    "            phi1, psi1: tensors of shape [n1]\n",
    "            phi2, psi2: tensors of shape [n2]\n",
    "        Returns:\n",
    "            Tensor of shape [n1, n2]\n",
    "        \"\"\"\n",
    "        # Using broadcasting:\n",
    "        diff_phi = phi1.unsqueeze(1) - phi2.unsqueeze(0)  # shape: [n1, n2]\n",
    "        diff_psi = psi1.unsqueeze(1) - psi2.unsqueeze(0)    # shape: [n1, n2]\n",
    "        expr = 2 - torch.cos(diff_phi) - torch.cos(diff_psi)\n",
    "        k_val = (self.sigma_f ** 2) * torch.exp(-expr / (self.l ** 2))\n",
    "        return k_val\n",
    "\n",
    "    def compute_K_grad(self):\n",
    "        \"\"\"\n",
    "        Computes the (2N x 2N) covariance matrix of the gradient observations using the second derivatives.\n",
    "        \n",
    "        For training points i and j:\n",
    "          K_phi_phi = k * [ cos(phi_i - phi_j)/(l^2) - sin(phi_i - phi_j)^2/(l^4) ]\n",
    "          K_psi_psi = k * [ cos(psi_i - psi_j)/(l^2) - sin(psi_i - psi_j)^2/(l^4) ]\n",
    "          K_phi_psi = - k * sin(phi_i - phi_j)*sin(psi_i - psi_j)/(l^4)\n",
    "        \"\"\"\n",
    "        diff_phi = self.phi_train.unsqueeze(1) - self.phi_train.unsqueeze(0)  # shape: [N, N]\n",
    "        diff_psi = self.psi_train.unsqueeze(1) - self.psi_train.unsqueeze(0)  # shape: [N, N]\n",
    "        \n",
    "        k_val = self.kernel(self.phi_train, self.psi_train,\n",
    "                            self.phi_train, self.psi_train)  # shape: [N, N]\n",
    "        \n",
    "        K_phi_phi = k_val * (torch.cos(diff_phi) / (self.l ** 2) -\n",
    "                             (torch.sin(diff_phi) ** 2) / (self.l ** 4))\n",
    "        K_psi_psi = k_val * (torch.cos(diff_psi) / (self.l ** 2) -\n",
    "                             (torch.sin(diff_psi) ** 2) / (self.l ** 4))\n",
    "        K_phi_psi = - k_val * (torch.sin(diff_phi) * torch.sin(diff_psi)) / (self.l ** 4)\n",
    "        \n",
    "       \n",
    "        top = torch.cat([K_phi_phi, K_phi_psi], dim=1)     # shape: [N, 2N]\n",
    "        bottom = torch.cat([K_phi_psi, K_psi_psi], dim=1)    # shape: [N, 2N]\n",
    "        K = torch.cat([top, bottom], dim=0)                   # shape: [2N, 2N]\n",
    "        return K\n",
    "\n",
    "    def log_marginal_likelihood(self, noise=1e-6):\n",
    "        \"\"\"\n",
    "        Computes the log marginal likelihood (LML) using the gradient observations.\n",
    "        Data vector d is (2N x 1) stacked as [grad_phi; grad_psi].\n",
    "        \"\"\"\n",
    "        d = torch.cat([self.grad_phi_train, self.grad_psi_train], dim=0).view(2 * self.N, 1)\n",
    "        K = self.compute_K_grad() + noise * torch.eye(2 * self.N, dtype=dtype, device=device)\n",
    "        \n",
    "        L = torch.cholesky(K)\n",
    "        alpha = torch.cholesky_solve(d, L)\n",
    "        logdetK = 2 * torch.sum(torch.log(torch.diag(L)))\n",
    "        ll = -0.5 * (d.t() @ alpha) - 0.5 * logdetK - (2 * self.N / 2) * torch.log(2 * torch.pi)\n",
    "        return ll.squeeze()\n",
    "\n",
    "    def optimize_hyperparameters(self, n_iter=2000, lr=0.01):\n",
    "        \"\"\"\n",
    "        Optimizes the hyperparameters (sigma_f and l) by maximizing the log marginal likelihood.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam([self.sigma_f, self.l], lr=lr)\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            optimizer.zero_grad()\n",
    "            ll = self.log_marginal_likelihood()\n",
    "            loss = -ll  # maximize LML <==> minimize negative LML\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i:4d}, Loss: {loss.item():.5f}, sigma_f: {self.sigma_f.item():.5f}, l: {self.l.item():.5f}\")\n",
    "\n",
    "    def predict(self, test_input: torch.Tensor, noise=1e-6):\n",
    "        \"\"\"\n",
    "        Given a batch of test inputs as a torch.Tensor of shape (M, 2) with columns [phi, psi],\n",
    "        predicts the function value at each test point using the predictive mean:\n",
    "        \n",
    "            μ = k_*^T K^{-1} d\n",
    "        \n",
    "        where k_* is the cross-covariance between the test function value and the training\n",
    "        gradient observations.\n",
    "        \n",
    "        Returns:\n",
    "            A torch.Tensor of shape (M, 1) with the predicted function values.\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(test_input):\n",
    "            raise ValueError(\"Test input must be a torch.Tensor.\")\n",
    "        if test_input.dim() != 2 or test_input.shape[1] != 2:\n",
    "            raise ValueError(f\"Test input must have shape [N, 2], got {test_input.shape}\")\n",
    "        \n",
    "        if self.alpha is None:\n",
    "            K = self.compute_K_grad() + noise * torch.eye(2 * self.N, dtype=dtype, device=device)\n",
    "            L = torch.cholesky(K)\n",
    "            d = torch.cat([self.grad_phi_train, self.grad_psi_train], dim=0).view(2 * self.N, 1)\n",
    "            self.alpha = torch.cholesky_solve(d, L).detach()\n",
    "        \n",
    "        M = test_input.shape[0]\n",
    "        phi_star = test_input[:, 0]  # shape: (M,)\n",
    "        psi_star = test_input[:, 1]  # shape: (M,)\n",
    "        \n",
    "        k_val = self.kernel(phi_star, psi_star, self.phi_train, self.psi_train)\n",
    "        \n",
    "        diff_phi = phi_star.unsqueeze(1) - self.phi_train.unsqueeze(0)  # shape: (M, N)\n",
    "        diff_psi = psi_star.unsqueeze(1) - self.psi_train.unsqueeze(0)    # shape: (M, N)\n",
    "        \n",
    "        k_grad_phi = (torch.sin(diff_phi) / (self.l ** 2)) * k_val  # shape: (M, N)\n",
    "        k_grad_psi = (torch.sin(diff_psi) / (self.l ** 2)) * k_val   # shape: (M, N)\n",
    "        \n",
    "        k_star = torch.cat([k_grad_phi, k_grad_psi], dim=1)\n",
    "        \n",
    "        # Compute the predictive mean:\n",
    "        # For each test point m, μ_m = k_star[m, :] @ alpha (alpha has shape (2N, 1)).\n",
    "        pred_mean = k_star @ self.alpha  # shape: (M, 1)\n",
    "        return pred_mean\n",
    "\n",
    "\n",
    "FES = GPWithGradients(torch.tensor(data, dtype = torch.float64, device = device))\n",
    "# gp_model.optimize_hyperparameters(n_iter=2000, lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict at a new test input given as a numpy array of shape (2,): [phi, psi].\n",
    "test_input = torch.tensor(np.array([[1.0, 2.0]], dtype=np.float64), device = device)\n",
    "pred_value = FES(test_input)\n",
    "print(f\"Predicted function value F({test_input[0,0]}, {test_input[0,1]}) = {pred_value}\")\n",
    "test_input = torch.tensor(np.array([[1.0 + 2*np.pi, 2.0]], dtype=np.float64), device = device)\n",
    "pred_value = FES(test_input)\n",
    "print(f\"Predicted function value F({test_input[0,0]}, {test_input[0,1]}) = {pred_value}\")\n",
    "test_input = torch.tensor(np.array([[1.0, 2.0 + 2*np.pi]], dtype=np.float64), device = device)\n",
    "pred_value = FES(test_input)\n",
    "print(f\"Predicted function value F({test_input[0,0]}, {test_input[0,1]}) = {pred_value}\")\n",
    "test_input = torch.tensor(np.array([[1.0+ 2*np.pi, 2.0 + 2*np.pi]], dtype=np.float64), device = device)\n",
    "pred_value = FES(test_input)\n",
    "print(f\"Predicted function value F({test_input[0,0]}, {test_input[0,1]}) = {pred_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def visualize_model(model, device, title=\"Potential Surface\", resolution=100):\n",
    "    \"\"\"\n",
    "    Visualizes the potential energy surface (PES) predicted by the model.\n",
    "    \n",
    "    Constructs a grid over [-π, π]^2 for (phi, psi) and feeds the raw angles\n",
    "    to the model. The model internally extracts the periodic features.\n",
    "    \n",
    "    Two plots are generated:\n",
    "      - a 3D surface plot,\n",
    "      - a 2D contour plot.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Create a grid for phi and psi over [-π, π]\n",
    "        phi_vals = torch.linspace(-np.pi, np.pi, resolution, device=device, dtype=torch.float64)\n",
    "        psi_vals = torch.linspace(-np.pi, np.pi, resolution, device=device, dtype=torch.float64)\n",
    "        phi_mesh, psi_mesh = torch.meshgrid(phi_vals, psi_vals, indexing='ij')\n",
    "        # Stack the raw angles to form inputs of shape [resolution^2, 2]\n",
    "        coords = torch.stack([phi_mesh.flatten(), psi_mesh.flatten()], dim=1).double()\n",
    "        print(\"Coordinates shape:\", coords.shape)\n",
    "        \n",
    "        V_pred = model(coords).reshape(phi_mesh.shape).cpu().numpy()\n",
    "        V_min = np.min(V_pred)\n",
    "        V_pred = V_pred - V_min\n",
    "    \n",
    "    phi_np = phi_mesh.cpu().numpy()\n",
    "    psi_np = psi_mesh.cpu().numpy()\n",
    "\n",
    "    # Create the plot.\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    norm = mpl.colors.TwoSlopeNorm(vmin=V_pred.min(), vcenter=50, vmax=V_pred.max())\n",
    "    \n",
    "    cp = plt.contourf(phi_np, psi_np, V_pred, levels=100, cmap=mpl.cm.jet, norm=norm)\n",
    "    plt.colorbar(cp)\n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title(title)\n",
    "    \n",
    "    return phi_np, psi_np, V_pred\n",
    "\n",
    "phi_np, psi_np, V_pred = visualize_model(FES, device=device, resolution=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def visualize_model_zoomin(model, device, traj = None, traj_ref = None, title=\"Potential Surface\", resolution=100,\n",
    "                    xlim=(-1, 1), ylim=(-1, 1), scale_factor=5):\n",
    "    with torch.no_grad():\n",
    "        phi_vals = torch.linspace(xlim[0], xlim[1], resolution, device=device, dtype=torch.float64)\n",
    "        psi_vals = torch.linspace(ylim[0], ylim[1], resolution, device=device, dtype=torch.float64)\n",
    "        phi_mesh, psi_mesh = torch.meshgrid(phi_vals, psi_vals, indexing='ij')\n",
    "        \n",
    "        coords = torch.stack([phi_mesh.flatten(), psi_mesh.flatten()], dim=1).double()\n",
    "        print(\"Coordinates shape:\", coords.shape)\n",
    "        \n",
    "        V_pred = model(coords).reshape(phi_mesh.shape).cpu().numpy()\n",
    "        V_min = np.min(V_pred)\n",
    "        V_pred = V_pred - V_min\n",
    "    \n",
    "    phi_np = phi_mesh.cpu().numpy()\n",
    "    psi_np = psi_mesh.cpu().numpy()\n",
    "\n",
    "    domain_width = xlim[1] - xlim[0]\n",
    "    domain_height = ylim[1] - ylim[0]\n",
    "    \n",
    "    figsize = (domain_width * scale_factor, domain_height * scale_factor*0.8)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    norm = mpl.colors.TwoSlopeNorm(vmin=V_pred.min(), vcenter=60, vmax=V_pred.max())\n",
    "    \n",
    "    cp = plt.contourf(phi_np, psi_np, V_pred, levels=200, cmap=mpl.cm.jet, norm=norm)\n",
    "    if traj is not None:\n",
    "        plt.plot(traj[:, 0], traj[:, 1], \"r.\", markersize=5)\n",
    "    if traj_ref is not None:\n",
    "        plt.plot(traj_ref[:, 0], traj_ref[:, 1], \"k.\", markersize=5)\n",
    "    plt.colorbar(cp)\n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Zoom in to the specified region.\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()\n",
    "    \n",
    "    return phi_np, psi_np, V_pred\n",
    "\n",
    "phi_np, psi_np, V_pred = visualize_model_zoomin(FES, device=device, resolution=200,\n",
    "                                           xlim=(-np.pi, np.pi), ylim=(-np.pi, np.pi), scale_factor=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pickle\n",
    "from io import StringIO\n",
    "import sys\n",
    "import copy\n",
    "from torch.autograd import grad\n",
    "for K in range(3,4):\n",
    "    C5 = [-2.54247629,  2.76015096]\n",
    "    C7eq = [-1.39111302,  0.99116045]\n",
    "    C7ax = [ 1.05515806, -0.68824516]#\n",
    "    MEPpass_start_point = C7eq#[-1.36744612,  0.98019218]#[-1.36744612,  0.98019218]\n",
    "    MEPpass_end_point = C7ax#[-2.55186883,  2.86101161]#[ 1.0910803,  -0.89521033]\n",
    "    N = 20\n",
    "    #-------------------------------------------------\n",
    "    mycase = \"36_amber_vacuum\"\n",
    "    casenum = f\"beta0{K}_C7eqC7ax_N{N}\"\n",
    "\n",
    "    import os\n",
    "    base_path = '/Users/wuzhiyou/Code/StringNET/on_the_fly/Data/'\n",
    "\n",
    "    # Full path to the new folder\n",
    "    folder_path = os.path.join(base_path, casenum)\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    all_path = folder_path\n",
    "\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    path_s = os.path.join(folder_path, f\"{mycase}_s_{casenum}.pkl\")\n",
    "    path_path = os.path.join(folder_path, f\"{mycase}_path_{casenum}.pkl\")\n",
    "    path_cos = os.path.join(folder_path, f\"{mycase}_cos_{casenum}.pkl\")\n",
    "    path_energy = os.path.join(folder_path, f\"{mycase}_energy_{casenum}.pkl\")\n",
    "    path_force = os.path.join(folder_path, f\"{mycase}_force_{casenum}.pkl\")\n",
    "    path_cosloss = os.path.join(folder_path, f\"{mycase}_coslosss_{casenum}.pkl\")\n",
    "    path_lmax = os.path.join(folder_path, f\"{mycase}_lmax_{casenum}.pkl\")\n",
    "    path_lg = os.path.join(folder_path, f\"{mycase}_lg_{casenum}.pkl\")\n",
    "    #-------------------------------------------------\n",
    "    print(path_s)\n",
    "\n",
    "    plot_use_all = []\n",
    "\n",
    "    torch.set_printoptions(precision=10)\n",
    "\n",
    "    model_Parameters_name = all_path + \"model_MEP_\" + mycase[0:2] + \"_\" + casenum + \"pretrain\" + \".pth\"  # Path\n",
    "    model_name = all_path + \"model_MEP_\" + mycase + \"_\" + casenum + \".pth\"\n",
    "    model_init = all_path + \"model_MEP_\" + \"dw\" + \"_\" + \"2\" + \".pth\"\n",
    "\n",
    "    load_model = False\n",
    "    batches = 50  # iterations\n",
    "    beta = 0.1*K\n",
    "    print(f\"beta = {beta}\")\n",
    "    learning_rate = 2e-4\n",
    "    dimension = 2  \n",
    "\n",
    "    x_lb = -math.pi\n",
    "    x_ub = math.pi\n",
    "    y_lb = -math.pi\n",
    "    y_ub = math.pi\n",
    "\n",
    "    alpha1 = 0\n",
    "    alpha2 = 2\n",
    "    alpha3 = 0.5\n",
    "\n",
    "    nu = 0\n",
    "    ww = 10\n",
    "    lambda__ = 0\n",
    "\n",
    "    def flatten_tensor_list(tensor_list):\n",
    "        \"\"\"Flatten a list of tensors into a single 1D tensor.\"\"\"\n",
    "        return torch.cat([t.reshape(-1) for t in tensor_list])\n",
    "\n",
    "    def unflatten_vector(vec, tensor_list):\n",
    "        \"\"\"\n",
    "        Unflatten a 1D tensor into a list of tensors with shapes matching those in tensor_list.\n",
    "        \"\"\"\n",
    "        out_list = []\n",
    "        idx = 0\n",
    "        for t in tensor_list:\n",
    "            numel = t.numel()\n",
    "            out_list.append(vec[idx: idx+numel].view_as(t))\n",
    "            idx += numel\n",
    "        return out_list\n",
    "\n",
    "    # ------------------------------\n",
    "    #  Conjugate Gradient Solver\n",
    "    # ------------------------------\n",
    "    def conjugate_gradient(Gv_func, b, tol=1e-6, max_iter=50):\n",
    "        \"\"\"\n",
    "        Solve the linear system G*v = b for v using the Conjugate Gradient (CG) algorithm.\n",
    "        Gv_func: callable that returns G*v given any flat vector v.\n",
    "        b: right-hand side (flat vector).\n",
    "        \"\"\"\n",
    "        x = torch.zeros_like(b)\n",
    "        r = b - Gv_func(x)\n",
    "        p = r.clone()\n",
    "        rsold = torch.dot(r, r)\n",
    "        for i in range(max_iter):\n",
    "            Ap = Gv_func(p)\n",
    "            alpha = rsold / (torch.dot(p, Ap) + 1e-12)\n",
    "            x = x + alpha * p\n",
    "            r = r - alpha * Ap\n",
    "            rsnew = torch.dot(r, r)\n",
    "            control = torch.sqrt(rsnew)\n",
    "            if control < tol:\n",
    "                print(f\"k = {i}, control = {control}\")\n",
    "                break\n",
    "            p = r + (rsnew / rsold) * p\n",
    "            rsold = rsnew\n",
    "        print(f\"k = {i}, control = {control}\")\n",
    "        return x\n",
    "\n",
    "    def potential_tensor(q):\n",
    "        out = FES(q)\n",
    "        return out\n",
    "\n",
    "    def gradient_tensor(q):\n",
    "        p = potential_tensor(q)\n",
    "        return torch.autograd.grad(\n",
    "            outputs=p,\n",
    "            inputs=q,\n",
    "            grad_outputs=torch.ones_like(p),\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "    def gradient_md(q):\n",
    "        q = q.cpu().detach().numpy()\n",
    "        phi_deg_recovered = q[:, 0] / deg_to_rad\n",
    "        psi_deg_recovered = q[:, 1] / deg_to_rad\n",
    "        grid = np.column_stack((phi_deg_recovered, psi_deg_recovered, q))\n",
    "        grad = []\n",
    "        for g in grid:\n",
    "                restrained_simulation(g)\n",
    "                delete_backupfile()\n",
    "                phi_deg, psi_deg, phi, psi = g\n",
    "                dir_name = f\"phi_{phi_deg}_psi_{psi_deg}\"\n",
    "                grad_phi, grad_psi = FES_gradient(g)\n",
    "                grad.append(np.array([grad_phi, grad_psi]))\n",
    "    #    grad\n",
    "        g = np.stack(grad)\n",
    "        out = torch.tensor(g, dtype = torch.float64).to(device)\n",
    "        subprocess.run(\n",
    "        'find . -type d -name \"phi_*\" -exec rm -rf {} +',\n",
    "        shell=True,\n",
    "        )\n",
    "        return out\n",
    "\n",
    "    def fig_cos_V_force(i, s, cos, g, x_pred_list, norm_F_perp):\n",
    "        v = potential_tensor(x_pred).cpu().detach().numpy()\n",
    "        force = np.sqrt(np.sum(g * g, axis=1))\n",
    "        sin_square = 1 - cos**2\n",
    "\n",
    "        mask = sin_square > 0.9  # Boolean mask for entries greater than 0.9\n",
    "\n",
    "        # Separate s based on mask\n",
    "        s_high = s[mask]\n",
    "        s_low = s[~mask]\n",
    "\n",
    "        # Separate sin_square based on mask\n",
    "        cos_high = cos[mask]\n",
    "        cos_low = cos[~mask]\n",
    "        sin_square_high = sin_square[mask]\n",
    "        sin_square_low = sin_square[~mask]\n",
    "\n",
    "        num_subplots = 5\n",
    "\n",
    "        # Define the desired size of each subplot in inches (e.g., 4x4 inches per subplot)\n",
    "        subplot_size = 4\n",
    "\n",
    "        # Calculate the overall figure size\n",
    "        fig_width = subplot_size * num_subplots\n",
    "        fig_height = subplot_size\n",
    "\n",
    "        # Create the figure with the calculated size\n",
    "        fig, axs = plt.subplots(1, num_subplots, figsize=(fig_width, fig_height))\n",
    "\n",
    "        # Subplot 1: Cosine\n",
    "        axs[0].plot(s_low, cos_low, 'b.')\n",
    "        axs[0].plot(s_high, cos_high, 'r.')\n",
    "        axs[0].set_ylim(-1.1, 1.1)\n",
    "        axs[0].set_title(\"$cos$\")\n",
    "        axs[0].set_box_aspect(1)  \n",
    "        # Subplot 2: l3(1-Cos^2)\n",
    "        axs[1].plot(s_low, sin_square_low, 'b.')\n",
    "        axs[1].plot(s_high, sin_square_high, 'r.')\n",
    "        axs[1].set_ylim(-0.1, 1.1)\n",
    "        axs[1].set_title(\"l3 $(1-cos^2)$\")\n",
    "        axs[1].set_box_aspect(1)  \n",
    "        # Subplot 3: Energy\n",
    "        axs[2].plot(s, v, 'b.')\n",
    "        axs[2].set_title(\"$Energy$\")\n",
    "        axs[2].set_box_aspect(1)  \n",
    "        # Subplot 4: Force\n",
    "        axs[3].plot(s, force, 'b.')\n",
    "        axs[3].set_title(\"$Force$\")\n",
    "        axs[3].set_box_aspect(1)  \n",
    "        # Subplot 5: Norm of F^{\\perp}\n",
    "        axs[4].plot(s, norm_F_perp, 'b.')\n",
    "        axs[4].set_title(\"norm of $F^{\\perp}$\")\n",
    "        axs[4].set_box_aspect(1)  \n",
    "        # Adjust the layout to avoid overlap\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if i % 10000 == 0 or i == 1 or i == 500 or i == 1000 or i == 2000 or i == 5000:\n",
    "            file_path = os.path.join(folder_path, \"Dot_plot_\" + \"Iter\" + str(i) + \"_\" + mycase + \"_\" + casenum + \".eps\")\n",
    "            plt.savefig(file_path)\n",
    "\n",
    "    def fig_loss_batch(loss, loss_1, loss_3, loss_ref, loss_1_ref, loss_3_ref, loss_EL, loss_EL_ref):\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "        axes[0].plot(loss, color=\"#B41830\", linestyle='-')\n",
    "        axes[0].plot(loss_ref, color=\"#2218B4\", linestyle='-')\n",
    "        axes[0].set_xlabel(\"Batches\")\n",
    "        axes[0].set_ylabel(\"Loss\")\n",
    "        axes[0].set_title(\"Loss vs Loss_ref\")\n",
    "\n",
    "        axes[1].plot(loss_1, color=\"#B41830\", linestyle='-')\n",
    "        axes[1].plot(loss_1_ref, color=\"#2218B4\", linestyle='-')\n",
    "        axes[1].set_xlabel(\"Batches\")\n",
    "        axes[1].set_ylabel(\"Loss_1\")\n",
    "        axes[1].set_title(\"Loss_1 vs Loss_1_ref\")\n",
    "\n",
    "        axes[2].plot(loss_3, color=\"#B41830\", linestyle='-')\n",
    "        axes[2].plot(loss_3_ref, color=\"#2218B4\", linestyle='-')\n",
    "        axes[2].set_xlabel(\"Batches\")\n",
    "        axes[2].set_ylabel(\"Loss_3\")\n",
    "        axes[2].set_title(\"Loss_3 vs Loss_3_ref\")\n",
    "\n",
    "\n",
    "        axes[3].semilogy(loss_EL, color=\"#B41830\", linestyle='-')\n",
    "        axes[3].semilogy(loss_EL_ref, color=\"#2218B4\", linestyle='-')\n",
    "        axes[3].set_xlabel(\"Batches\")\n",
    "        axes[3].set_ylabel(\"Loss_Euler_Lagrangian\")\n",
    "        axes[3].set_title(\"Loss_EL vs Loss_EL_ref\")\n",
    "\n",
    "        # Adjust subplots for a neat layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def fig_cos(plt_batch, cos_batch, lmax_batch, lg_batch):\n",
    "        fig = plt.figure(figsize=(15, 4))\n",
    "        fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.semilogy(plt_batch, cos_batch, 'b-')\n",
    "        plt.xlabel(\"Batches\")\n",
    "        plt.ylabel(\"$\\int 1-Cos^2(force,tangent) ds$\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(plt_batch, lmax_batch, 'b-')\n",
    "        plt.xlabel(\"Batches\")\n",
    "        plt.ylabel(\"$lmax$\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(plt_batch, lg_batch, 'b-')\n",
    "        plt.xlabel(\"Batches\")\n",
    "        plt.ylabel(\"$l_g$\")\n",
    "\n",
    "\n",
    "    x = np.linspace(x_lb, x_ub, num=51, endpoint=True)\n",
    "    y = np.linspace(y_lb, y_ub, num=51, endpoint=True)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    X_new = X.reshape(-1, 1)\n",
    "    Y_new = Y.reshape(-1, 1)\n",
    "    XY = np.hstack((X_new, Y_new))\n",
    "    for i in range(dimension - 2):\n",
    "        XY = np.hstack((XY, np.zeros(X_new.shape)))\n",
    "\n",
    "    XY_tensor = torch.from_numpy(XY)\n",
    "    X_list = []\n",
    "    for i in range(dimension):\n",
    "        X_list.append(XY_tensor[:, i:i+1].to(device))\n",
    "        print(XY_tensor[:, i:i+1].to(device).shape)\n",
    "    X_list = torch.hstack(X_list)\n",
    "    Z = potential_tensor(X_list)\n",
    "    Z_new = Z.reshape(X.shape).cpu().detach().numpy()\n",
    "    Z_new = Z_new - np.min(Z_new)\n",
    "    def fig_countour(k, x_plot, x_plot_ref, xlim, ylim, scale_factor=1.3):\n",
    "        visualize_model_zoomin(FES, device=device, traj = x_plot, traj_ref = x_plot_ref, resolution=200,\n",
    "                                            xlim=xlim, ylim=ylim, scale_factor=scale_factor)\n",
    "\n",
    "        if k % 10000 == 0 or k == 1 or k == 500 or k == 1000 or k == 2000 or k == 5000:\n",
    "            file_path = os.path.join(folder_path, \"Contour_\" + \"Iter\" + str(k) + \"_\" + mycase + \"_\" + casenum + \".eps\")\n",
    "            plt.savefig(file_path)\n",
    "        \n",
    "\n",
    "    class ResNetBlock(nn.Module):\n",
    "        \"\"\"A single ResNet block with two linear layers and Tanh activations.\"\"\"\n",
    "        def __init__(self, hidden_size):\n",
    "            super(ResNetBlock, self).__init__()\n",
    "            self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.activation = nn.Tanh()\n",
    "            self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            residual = x\n",
    "            out = self.linear1(x)\n",
    "            out = self.activation(out)\n",
    "            out = self.linear2(out)\n",
    "            out = self.activation(out)\n",
    "            return out + residual\n",
    "        \n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self,st,ed):\n",
    "            super(NeuralNetwork, self).__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_tanh_stack = nn.Sequential(\n",
    "                nn.Linear(1, 50),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(50, 50),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(50, 50),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(50, 50),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(50, dimension)\n",
    "            )\n",
    "            self.startpoint=Variable(torch.from_numpy(np.array([st])),requires_grad=False).to(device)\n",
    "            self.endpoint=Variable(torch.from_numpy(np.array([ed])),requires_grad=False).to(device)\n",
    "\n",
    "\n",
    "        def forward(self, s):\n",
    "            s = self.flatten(s)\n",
    "            x_pred = self.linear_tanh_stack(s)\n",
    "            out=s*(1-s)*x_pred + (1-s)*self.startpoint + s*self.endpoint\n",
    "            return out\n",
    "   \n",
    "\n",
    "        def gradient(self, s):\n",
    "            \"\"\"\n",
    "            Compute the gradient of the network's output with respect to the input s.\n",
    "            This function first ensures that s has requires_grad=True, computes the output,\n",
    "            and then computes the gradient using torch.autograd.grad.\n",
    "            \"\"\"\n",
    "\n",
    "            out = self.forward(s)\n",
    "            grad0 = torch.autograd.grad(\n",
    "                outputs=out[:,0], \n",
    "                inputs=s, \n",
    "                grad_outputs=torch.ones_like(out[:,0]),\n",
    "                create_graph=True, \n",
    "                retain_graph=True\n",
    "            )[0]\n",
    "            grad1 = torch.autograd.grad(\n",
    "                outputs=out[:,1], \n",
    "                inputs=s, \n",
    "                grad_outputs=torch.ones_like(out[:,1]),\n",
    "                create_graph=True, \n",
    "                retain_graph=True\n",
    "            )[0]\n",
    "            grad = torch.hstack((grad0,grad1))\n",
    "            \n",
    "            return grad\n",
    "        def hessian(self, s):\n",
    "            # Ensure s has requires_grad enabled.\n",
    "            \n",
    "            out = self.gradient(s)\n",
    "            \n",
    "            # First derivatives for each output component.\n",
    "            grad0 = torch.autograd.grad(\n",
    "                outputs=out[:, 0],\n",
    "                inputs=s,\n",
    "                grad_outputs=torch.ones_like(out[:, 0]),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )[0]\n",
    "            \n",
    "            grad1 = torch.autograd.grad(\n",
    "                outputs=out[:, 1],\n",
    "                inputs=s,\n",
    "                grad_outputs=torch.ones_like(out[:, 1]),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )[0]\n",
    "            grad = torch.hstack((grad0,grad1))\n",
    "            \n",
    "            return grad\n",
    "        def partial_s(self, s, input):\n",
    "            # Ensure s has requires_grad enabled.\n",
    "            grad0 = torch.autograd.grad(\n",
    "                outputs=input[:, 0],\n",
    "                inputs=s,\n",
    "                grad_outputs=torch.ones_like(input[:, 0]),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )[0]\n",
    "            \n",
    "            grad1 = torch.autograd.grad(\n",
    "                outputs=input[:, 1],\n",
    "                inputs=s,\n",
    "                grad_outputs=torch.ones_like(input[:, 1]),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )[0]\n",
    "            grad = torch.hstack((grad0,grad1))\n",
    "            \n",
    "            return grad\n",
    "    def compute_jacobian_list(model, s, theta_list):\n",
    "        \"\"\"\n",
    "        For each integration point s[i] (i=1,...,N), compute the Jacobian J_i = d(φ(s_i))/dθ.\n",
    "        Each Jacobian is stored as a tensor of shape [output_dim, P],\n",
    "        where P is the total number of parameters (flattened).\n",
    "        \"\"\"\n",
    "        N = s.shape[0]\n",
    "        jacobian_list = []\n",
    "        # Total number of parameters P\n",
    "        P = sum(p.numel() for p in theta_list)\n",
    "        output_dim = model(s[:1]).shape[1]  # e.g., dimension of output (here 12)\n",
    "        for i in range(N):\n",
    "            s_i = s[i:i+1]  # shape (1,1)\n",
    "            x_pred_i = model(s_i)  # shape (1, output_dim)\n",
    "            # For each output component, compute gradient (a row vector of shape [P])\n",
    "            rows = []\n",
    "            for j in range(output_dim):\n",
    "                grads = grad(outputs=x_pred_i[:, j], inputs=theta_list,\n",
    "                            grad_outputs=torch.ones_like(x_pred_i[:, j]),\n",
    "                            retain_graph=True, create_graph=False)\n",
    "                # Flatten all gradients to a single row\n",
    "                row = flatten_tensor_list(grads)\n",
    "                rows.append(row)\n",
    "            # Stack rows to produce J_i of shape (output_dim, P)\n",
    "            J_i = torch.stack(rows, dim=0)\n",
    "            jacobian_list.append(J_i)\n",
    "        return jacobian_list\n",
    "\n",
    "    def natural_gradient_operator(v_flat, jacobian_list, damping=0.0):\n",
    "        \"\"\"\n",
    "        Given a flat vector v_flat of shape [P], compute:\n",
    "        G v = (1/N) * sum_{i=1}^N (J_i^T (J_i v_flat))  + damping * v_flat\n",
    "        where each J_i comes from jacobian_list.\n",
    "        \"\"\"\n",
    "        total = torch.zeros_like(v_flat)\n",
    "        N = len(jacobian_list)\n",
    "        for J in jacobian_list:\n",
    "            # J: shape [output_dim, P], v_flat: [P]\n",
    "            jv = torch.matmul(J, v_flat)           # shape: [output_dim]\n",
    "            total += torch.matmul(J.t(), jv)         # shape: [P]\n",
    "        return total / N + damping * v_flat\n",
    "\n",
    "    def weights_init_zero(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.zeros_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    print(f\"Using {device} device to train\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "        \n",
    "    model = NeuralNetwork(MEPpass_start_point, MEPpass_end_point).to(device).double()\n",
    "    if load_model == True:\n",
    "        state_dict = torch.load(f\"/Users/wuzhiyou/Code/Server/StringNET/Data/dataset3/beta0{K}_C7eqC7ax_ref2_N100/model_MEP_36_amber_vacuum_beta0{K}_C7eqC7ax_ref2_N100_60000.pth\", map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "    model_ref = NeuralNetwork(MEPpass_start_point, MEPpass_end_point).to(device).double()\n",
    "    model_ref.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "    save_data = False\n",
    "    output_results = True\n",
    "\n",
    "    loss_batch = []\n",
    "    plt_batch = []\n",
    "    cos_batch = []\n",
    "\n",
    "    lg_batch = []\n",
    "    lmax_batch = []\n",
    "\n",
    "    index_seq = []\n",
    "    loss_seq = []\n",
    "    loss1_seq = []\n",
    "    loss2_seq = []\n",
    "    loss3_seq = []\n",
    "    loss4_seq = []\n",
    "    loss7_seq = []\n",
    "    lossimf_seq = []\n",
    "    loss_EL_seq = []\n",
    "\n",
    "    loss_ref_seq = []\n",
    "    loss1_ref_seq = []\n",
    "    loss2_ref_seq = []\n",
    "    loss3_ref_seq = []\n",
    "    loss_EL_ref_seq = []\n",
    "\n",
    "    lr_seq = []\n",
    "\n",
    "    data = {}\n",
    "    generated_samples = None\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr = 1e-4)\n",
    "    optimizer_ref = torch.optim.Adam(list(model_ref.parameters()), lr = 1e-4)\n",
    "\n",
    "    for i in range(1, batches + 1):\n",
    "        #---------------n1-------------------\n",
    "        if i < 20000:\n",
    "            alpha1 = 10\n",
    "            alpha2 = 10\n",
    "            alpha3 = 1\n",
    "        else:\n",
    "            alpha1 = 0.1\n",
    "            alpha2 = 10\n",
    "            alpha3 = 1\n",
    "\n",
    "        if i < 80:\n",
    "            learning_rate = 5e-4\n",
    "        elif 80 <= i < 50000:\n",
    "            learning_rate = 2e-4\n",
    "        elif 50000 <= i <= 200000:\n",
    "            learning_rate = 2e-4\n",
    "        else:\n",
    "            learning_rate = 1e-4\n",
    "        \n",
    "        # learning_rate = 2e-5\n",
    "        lr_seq.append(learning_rate)\n",
    "\n",
    "    \n",
    "\n",
    "        s_np = np.random.uniform(0.001,0.999,(N,1))\n",
    "            \n",
    "        s = torch.tensor(s_np, requires_grad=True, dtype = torch.float64).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        x_pred_ref = model_ref(s)\n",
    "        #-----------------loss_1-----------------\n",
    "        v_ref = potential_tensor(x_pred_ref)\n",
    "        \n",
    "        partial_s_x_ref = model_ref.gradient(s)\n",
    "        partial_ss_x_ref = model_ref.hessian(s)\n",
    "        \n",
    "        norm_partial_s_phi_square_ref = torch.sum(partial_s_x_ref * partial_s_x_ref, axis=1, keepdim=True)\n",
    "        norm_partial_s_phi_ref = norm_partial_s_phi_square_ref**0.5\n",
    "        g_ref = gradient_tensor(x_pred_ref)\n",
    "        norm_g_square_ref = torch.sum(g_ref * g_ref, axis=1, keepdim=True)\n",
    "        norm_g_ref = norm_g_square_ref**0.5\n",
    "\n",
    "        loss_1_ref = 1 / beta * torch.log(torch.mean(torch.exp(v_ref * beta) * norm_partial_s_phi_ref))\n",
    "        \n",
    "        \n",
    "        cos_ref = torch.sum(partial_s_x_ref * g_ref, axis=1, keepdim=True) / (norm_g_ref * norm_partial_s_phi_ref)\n",
    "        cos2_ref = cos_ref**2\n",
    "\n",
    "        \n",
    "\n",
    "        loss_2_ref = torch.mean(1 - cos2_ref)\n",
    "        #-----------------loss_4-----------------\n",
    "        F_ref = g_ref\n",
    "        inner_product_ref = torch.sum(partial_s_x_ref * g_ref, axis=1, keepdim=True)\n",
    "        F_perpendicular_ref = F_ref - (inner_product_ref / (norm_partial_s_phi_square_ref)) * partial_s_x_ref\n",
    "        norm_F_perp_square_ref = torch.sum(F_perpendicular_ref * F_perpendicular_ref, axis=1, keepdim=True)\n",
    "        norm_F_perp_ref = norm_F_perp_square_ref**0.5\n",
    "        loss_4_ref = torch.mean(norm_F_perp_square_ref)\n",
    "\n",
    "        one_minus_cos2_ref = 1 - cos2_ref\n",
    "        one_minus_cos2_flat_ref = one_minus_cos2_ref.view(-1)\n",
    "\n",
    "        # Start with the initial threshold\n",
    "        threshold = 0.9\n",
    "        filtered_values = torch.tensor([])  # Initialize an empty tensor for filtered values\n",
    "        loss_g_ref = torch.mean(norm_partial_s_phi_ref*norm_g_ref)\n",
    "\n",
    "\n",
    "        tau_ref = partial_s_x_ref/norm_partial_s_phi_ref\n",
    "        dtau_ds_ref = model.partial_s(s, tau_ref)\n",
    "        dI_dphi_ref = F_perpendicular_ref - 1/beta*dtau_ds_ref/norm_partial_s_phi_ref\n",
    "        EL_ref = torch.sum(dI_dphi_ref * dI_dphi_ref, axis=1, keepdim=True)\n",
    "        loss_Euler_Lag_ref = torch.mean(EL_ref)\n",
    "\n",
    "    \n",
    "        cons_ref = torch.sum(partial_s_x_ref * partial_ss_x_ref, axis=1, keepdim=True)\n",
    "        loss_3_ref = torch.mean(cons_ref**2)\n",
    "\n",
    "        loss_ref = loss_1_ref + 0.1*loss_3_ref\n",
    "        optimizer_ref.zero_grad()\n",
    "        loss_here_ref = 0.1*loss_3_ref\n",
    "        loss_here_ref.backward(retain_graph=True)  # retain_graph if you'll compute additional grads\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        ds = 1.0 / N  # Integration weight\n",
    "        theta_list_ref = list(model_ref.parameters())\n",
    "        custom_gradients_ref = torch.autograd.grad(\n",
    "            outputs=x_pred_ref,\n",
    "            inputs=theta_list_ref,\n",
    "            grad_outputs=dI_dphi_ref,      # This is δI/δφ(s)\n",
    "            retain_graph=True,         # Retain the graph if you need further grad computations\n",
    "            create_graph=False         # No need for higher-order derivatives in this example\n",
    "        )\n",
    "        custom_gradients_ref = [grad * ds for grad in custom_gradients_ref]\n",
    "        with torch.no_grad():\n",
    "            for param, custom_grad in zip(theta_list_ref, custom_gradients_ref):\n",
    "                if param.grad is None:\n",
    "                    param.grad = custom_grad.clone()\n",
    "                else:\n",
    "                    param.grad.add_(custom_grad)  # Gradient descent uses grad; the optimizer subtracts it.\n",
    "        optimizer_ref.step()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        x_pred = model(s)\n",
    "        x_pred_list = []\n",
    "        for n in range(dimension):\n",
    "            x_pred_list.append(x_pred[:, n:n+1])\n",
    "        #-----------------loss_1-----------------\n",
    "        v = potential_tensor(x_pred)\n",
    "        \n",
    "        partial_s_x = model.gradient(s)\n",
    "        partial_ss_x = model.hessian(s)\n",
    "        \n",
    "        norm_partial_s_phi_square = torch.sum(partial_s_x * partial_s_x, axis=1, keepdim=True)\n",
    "        norm_partial_s_phi = norm_partial_s_phi_square**0.5\n",
    "\n",
    "        loss_1 = 1 / beta * torch.log(torch.mean(torch.exp(v * beta) * norm_partial_s_phi))\n",
    "        \n",
    "        g = gradient_md(x_pred)\n",
    "        # else:\n",
    "        #     g = gradient_tensor(x_pred)\n",
    "        \n",
    "        cos = torch.sum(partial_s_x * g, axis=1, keepdim=True) / (\n",
    "            torch.sum(g * g, axis=1, keepdim=True)**0.5 * torch.sum(partial_s_x * partial_s_x, axis=1, keepdim=True)**0.5)\n",
    "        cos2 = cos**2\n",
    "\n",
    "        norm_g_square = torch.sum(g * g, axis=1, keepdim=True)\n",
    "        norm_g = norm_g_square**0.5\n",
    "\n",
    "        loss_2 = torch.mean(1 - cos2)\n",
    "        #-----------------loss_4-----------------\n",
    "        F = g\n",
    "        inner_product = torch.sum(partial_s_x * g, axis=1, keepdim=True)\n",
    "        F_perpendicular = F - (inner_product / (norm_partial_s_phi_square)) * partial_s_x\n",
    "        norm_F_perp_square = torch.sum(F_perpendicular * F_perpendicular, axis=1, keepdim=True)\n",
    "        norm_F_perp = norm_F_perp_square**0.5\n",
    "        loss_4 = torch.mean(norm_F_perp_square)\n",
    "\n",
    "        one_minus_cos2 = 1 - cos2\n",
    "        one_minus_cos2_flat = one_minus_cos2.view(-1)\n",
    "\n",
    "        # Start with the initial threshold\n",
    "        threshold = 0.9\n",
    "        filtered_values = torch.tensor([])  # Initialize an empty tensor for filtered values\n",
    "        loss_g = torch.mean(norm_partial_s_phi*norm_g)\n",
    "\n",
    "        tau = partial_s_x/norm_partial_s_phi\n",
    "        dtau_ds = model.partial_s(s, tau)\n",
    "        dI_dphi = F_perpendicular - 1/beta*dtau_ds/norm_partial_s_phi\n",
    "\n",
    "        EL = torch.sum(dI_dphi * dI_dphi, axis=1, keepdim=True)\n",
    "        loss_Euler_Lag = torch.mean(EL)\n",
    "    \n",
    "        cons = torch.sum(partial_s_x * partial_ss_x, axis=1, keepdim=True)\n",
    "        loss_3 = torch.mean(cons**2)\n",
    "\n",
    "        loss = loss_1 + 0.1*loss_3\n",
    "        optimizer.zero_grad()\n",
    "        loss_here = 0.1*loss_3\n",
    "        loss_here.backward(retain_graph=True)  # retain_graph if you'll compute additional grads\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        ds = 1.0 / N  # Integration weight\n",
    "        theta_list = list(model.parameters())\n",
    "        custom_gradients = torch.autograd.grad(\n",
    "            outputs=x_pred,\n",
    "            inputs=theta_list,\n",
    "            grad_outputs=dI_dphi,      # This is δI/δφ(s)\n",
    "            retain_graph=True,         # Retain the graph if you need further grad computations\n",
    "            create_graph=False         # No need for higher-order derivatives in this example\n",
    "        )\n",
    "\n",
    "        custom_gradients = [grad * ds for grad in custom_gradients]\n",
    "        with torch.no_grad():\n",
    "            for param, custom_grad in zip(theta_list, custom_gradients):\n",
    "                if param.grad is None:\n",
    "                    param.grad = custom_grad.clone()\n",
    "                else:\n",
    "                    param.grad.add_(custom_grad)  # Gradient descent uses grad; the optimizer subtracts it.\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        index_seq.append(i)\n",
    "        loss_seq.append(loss.detach().cpu().numpy())\n",
    "        loss1_seq.append(loss_1.detach().cpu().numpy())\n",
    "        loss2_seq.append(loss_2.detach().cpu().numpy())\n",
    "        loss3_seq.append(loss_3.detach().cpu().numpy())\n",
    "        loss4_seq.append(loss_4.detach().cpu().numpy())\n",
    "        loss_EL_seq.append(loss_Euler_Lag.detach().cpu().numpy())\n",
    "\n",
    "        loss_ref_seq.append(loss_ref.detach().cpu().numpy())\n",
    "        loss1_ref_seq.append(loss_1_ref.detach().cpu().numpy())\n",
    "        # loss2_seq.append(loss_2.detach().cpu().numpy())\n",
    "        loss3_ref_seq.append(loss_3_ref.detach().cpu().numpy())\n",
    "        loss_EL_ref_seq.append(loss_Euler_Lag_ref.detach().cpu().numpy())\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        if i % 1  == 0:\n",
    "            print(f'batches: {i + 1}')\n",
    "            print(f'loss: {loss.detach().cpu().numpy()}')\n",
    "\n",
    "\n",
    "        if i % 100 == 0 or i == 1 or i == 500 or i == 1000 or i == 2000 or i == 5000 or i > 0:\n",
    "        \n",
    "            fig_cos(plt_batch, cos_batch, lmax_batch, lg_batch)\n",
    "            fig_countour(i, x_pred.cpu().detach().numpy(), x_pred_ref.cpu().detach().numpy(), xlim=(-2, 2), ylim=(-2, 2))\n",
    "            fig_cos_V_force(i, s.cpu().detach().numpy(), cos.cpu().detach().numpy(), g.cpu().detach().numpy(), x_pred, norm_F_perp.cpu().detach().numpy())\n",
    "            fig_loss_batch(np.array(loss_seq),np.array(loss1_seq),np.array(loss3_seq),\n",
    "                        np.array(loss_ref_seq),np.array(loss1_ref_seq),np.array(loss3_ref_seq),\n",
    "                        np.array(loss_EL_seq),np.array(loss_EL_ref_seq))\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "            model_name = os.path.join(folder_path, f\"model_MEP_{mycase}_{casenum}_{i}.pth\")\n",
    "            # Save the PyTorch model state\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(f\"Saved PyTorch Model State to {model_name}\")\n",
    "\n",
    "            if i % 5000 == 0 or i == 1 or i == 100 or i == 200 or i == 500 or i == 1000 or i == 2000 or i == 5000 or i > 0:\n",
    "                data[\"loss_seq\"] = loss_seq\n",
    "                data[\"loss1_seq\"] = loss1_seq\n",
    "                data[\"loss2_seq\"] = loss2_seq\n",
    "                data[\"loss3_seq\"] = loss3_seq\n",
    "                data[\"loss4_seq\"] = loss4_seq\n",
    "                data[\"loss_seq\"] = loss_seq\n",
    "                data[\"loss1_seq\"] = loss1_seq\n",
    "                data[\"loss2_seq\"] = loss2_seq\n",
    "                data[\"loss3_seq\"] = loss3_seq\n",
    "                data[\"loss4_seq\"] = loss4_seq\n",
    "                data[\"loss_EL_seq\"] = loss_EL_seq\n",
    "\n",
    "                data[\"loss_ref_seq\"] = loss_ref_seq\n",
    "                data[\"loss1_ref_seq\"] = loss1_ref_seq\n",
    "                # data[\"loss2_seq\"] = loss2_seq\n",
    "                data[\"loss3_ref_seq\"] = loss3_ref_seq\n",
    "                # data[\"loss4_seq\"] = loss4_seq\n",
    "                data[\"loss_EL_ref_seq\"] = loss_EL_ref_seq\n",
    "            \n",
    "                data[\"loss7_seq\"] = loss7_seq\n",
    "                data[\"lossimf_seq\"] = lossimf_seq\n",
    "                data[\"lr_seq\"] = lr_seq\n",
    "                data[\"lr_seq\"] = lr_seq\n",
    "                if hasattr(model, 'module'):\n",
    "                    data[f'model_state_dict_{i}'] = copy.deepcopy(model.module.state_dict())\n",
    "                    data[f'model_ref_state_dict_{i}'] = copy.deepcopy(model_ref.module.state_dict())\n",
    "                else:\n",
    "                    data[f'model_state_dict_{i}'] = copy.deepcopy(model.state_dict())\n",
    "                    data[f'model_ref_state_dict_{i}'] = copy.deepcopy(model_ref.state_dict())\n",
    "                data[f\"x_pred_iter_{i}\"] = x_pred.cpu().detach().numpy()\n",
    "                data[f\"x_pred_ref_iter_{i}\"] = x_pred_ref.cpu().detach().numpy()\n",
    "                data[f\"dI_dphi_{i}\"] = dI_dphi.cpu().detach().numpy()\n",
    "                # Store additional variables\n",
    "                data[f\"s_iter_{i}\"] = s.cpu().detach().numpy()\n",
    "                data[f\"cos_iter_{i}\"] = cos.cpu().detach().numpy()\n",
    "                data[f\"g_iter_{i}\"] = g.cpu().detach().numpy()\n",
    "                # data[f\"x_pred_list_iter_{i}\"] = x_pred_list\n",
    "                data[f\"v_iter_{i}\"] = potential_tensor(x_pred).cpu().detach().numpy()\n",
    "                data[f\"norm_F_perp_iter_{i}\"] = norm_F_perp.cpu().detach().numpy()\n",
    "                data[f\"EL_{i}\"] = EL.cpu().detach().numpy()\n",
    "                data[f\"EL_ref{i}\"] = EL_ref.cpu().detach().numpy()\n",
    "                if save_data == True:\n",
    "                    \n",
    "                    filename = os.path.join(folder_path, mycase + \"_\" + casenum + '.data')\n",
    "                    with open(filename, 'wb') as file:\n",
    "                        pickle.dump(data, file)\n",
    "                    print(f\"Data saved successfully in {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nnpes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
